# Imbalanced-ML-Benchmarking-Repository
Systematic Benchmarking of Machine Learning Models on Imbalanced Datasets

```markdown
# Systematic Benchmarking of Machine Learning Models on Imbalanced Datasets

## ðŸ“Œ Overview
Many real-world ML problems such as **fraud detection**, **medical diagnosis**, and **customer churn prediction** involve **severely imbalanced datasets**.  
In such scenarios, **accuracy is misleading**, and naive model comparisons can lead to incorrect conclusions.

This repository provides a **research-grade, reproducible benchmarking framework** to evaluate machine learning models on imbalanced data using **robust metrics** and **statistical significance testing**.

---

## ðŸŽ¯ Objectives
- Benchmark ML models fairly on imbalanced datasets  
- Implement metrics suitable for skewed class distributions  
- Perform statistically valid model comparisons  
- Provide a clean, extensible evaluation pipeline  

---

## ðŸ§  Key Contributions
- Custom implementation of **G-Mean**, **MCC**, and **PR-AUC**
- **Stratified K-Fold Cross-Validation**
- **Wilcoxon Signed-Rank** and **Friedman tests** for significance
- Modular and reproducible experimental design

---
