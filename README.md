# Systematic Benchmarking of Machine Learning Models on Imbalanced Datasets

## ğŸ“Œ Overview

Many real-world machine learning problems such as **fraud detection**, **medical diagnosis**, and **customer churn prediction** involve **severely imbalanced datasets**. In these scenarios, traditional accuracy-based evaluation is misleading and can result in incorrect conclusions.

This repository provides a **research-grade, reproducible benchmarking framework** to evaluate machine learning models on imbalanced datasets using **robust metrics** and **statistical significance testing**.

---

## ğŸ¯ Objectives

* Benchmark machine learning models fairly on imbalanced datasets
* Implement metrics suitable for skewed class distributions
* Perform statistically valid model comparisons
* Provide a clean, extensible evaluation pipeline

---

## ğŸ§  Key Contributions

* Custom implementation of **G-Mean**, **Matthews Correlation Coefficient (MCC)**, and **PR-AUC**
* **Stratified K-Fold Cross-Validation** for reliable evaluation
* **Wilcoxon Signed-Rank** and **Friedman tests** for statistical significance
* Modular and reproducible experimental design

---

## ğŸ“‚ Repository Structure

```
imbalanced-ml-benchmark/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ loaders.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ logistic.py
â”‚   â”œâ”€â”€ random_forest.py
â”‚   â””â”€â”€ xgboost.py
â”‚
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ gmean.py
â”‚   â”œâ”€â”€ mcc.py
â”‚   â””â”€â”€ pr_auc.py
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ cross_validation.py
â”‚   â””â”€â”€ statistical_tests.py
â”‚
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ run_experiment.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Š Evaluation Metrics

### 1ï¸âƒ£ G-Mean

Balances sensitivity and specificity and is defined as:

[ G\text{-}Mean = \sqrt{TPR \times TNR} ]

### 2ï¸âƒ£ Matthews Correlation Coefficient (MCC)

A correlation-based metric robust to class imbalance:

[ MCC \in [-1, 1] ]

### 3ï¸âƒ£ Precisionâ€“Recall AUC (PR-AUC)

Preferred over ROC-AUC for highly imbalanced datasets, as it focuses on the minority class performance.

---

## ğŸ”„ Cross-Validation Strategy

* **Stratified K-Fold (k = 5)**
* Preserves class ratios in each fold
* Identical splits used across all models for fair comparison

---

## ğŸ“ˆ Statistical Significance Testing

To ensure that observed performance differences are **not due to randomness**, the following tests are used:

* **Wilcoxon Signed-Rank Test** for pairwise model comparison
* **Friedman Test** for comparing multiple models

A p-value < 0.05 indicates statistically significant differences.

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/imbalanced-ml-benchmark.git
cd imbalanced-ml-benchmark
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Download Dataset

Download the **UCI Credit Card Fraud Detection Dataset** and place it in:

```
data/raw/creditcard.csv
```

### 4ï¸âƒ£ Run Experiments

```bash
python experiments/run_experiment.py
```

---

## ğŸ§ª Models Implemented

* Logistic Regression (class-weighted)
* Random Forest (class-weighted)
* XGBoost (cost-sensitive learning)

---

## ğŸ“Œ Example Output

```
Logistic      | GMean=0.61 | MCC=0.39 | PR-AUC=0.36
RandomForest  | GMean=0.72 | MCC=0.55 | PR-AUC=0.52
XGBoost       | GMean=0.78 | MCC=0.63 | PR-AUC=0.61

Wilcoxon Test (RF vs XGB on PR-AUC)
p-value = 0.018
```

---

## ğŸ”¬ Future Extensions

* SMOTE vs cost-sensitive learning comparison
* Nemenyi post-hoc analysis
* Multi-class imbalance benchmarking
* Time-series imbalanced datasets
* LaTeX-ready result tables for publication

---

## ğŸ‘¤ Intended Audience

* Data Scientists
* Machine Learning Engineers
* ML Researchers
* MSc / PhD students

---

## ğŸ“œ License

MIT License

---

## â­ Citation

If you use this repository for academic or professional work, please cite the project.

